{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN9kysBqEC5TGE3+2uPph0K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install wandb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"coRqzWzY5MNq","executionInfo":{"status":"ok","timestamp":1710586396723,"user_tz":-540,"elapsed":9293,"user":{"displayName":"이진","userId":"07182137582732274273"}},"outputId":"8d9ec7fe-9405-4de5-851b-49f67d3130df"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting wandb\n","  Downloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/2.2 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n","  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Collecting sentry-sdk>=1.0.0 (from wandb)\n","  Downloading sentry_sdk-1.42.0-py2.py3-none-any.whl (263 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.5/263.5 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n","Collecting setproctitle (from wandb)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n","Successfully installed GitPython-3.1.42 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.42.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.4\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"WTwapP_P1oBl","executionInfo":{"status":"ok","timestamp":1710586397568,"user_tz":-540,"elapsed":850,"user":{"displayName":"이진","userId":"07182137582732274273"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from torchvision import models, transforms\n","from torch.utils.data import DataLoader, Dataset\n","from torchsummary import summary\n","from PIL import Image\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from tqdm import tqdm\n","import random\n","from tqdm import tqdm\n","import os\n","import matplotlib.pyplot as plt\n","import wandb\n","import logging\n","import math"]},{"cell_type":"code","source":["class CustomDataset(Dataset):\n","    def __init__(self, csv_file, image_folder, transform=None):\n","        self.df = pd.read_csv(csv_file)\n","        self.image_folder = image_folder\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.df['img_path'].iloc[idx].split(\"/\")[-1]\n","        img_path = os.path.join(self.image_folder, img_name)\n","        image = Image.open(img_path)\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image"],"metadata":{"id":"IwnetAYi1v4n","executionInfo":{"status":"ok","timestamp":1710586397568,"user_tz":-540,"elapsed":4,"user":{"displayName":"이진","userId":"07182137582732274273"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["transform = transforms.Compose(\n","    [\n","        transforms.Resize((256, 256)),\n","        transforms.CenterCrop((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]),\n","    ]\n",")"],"metadata":{"id":"SfD-IVRs2eFj","executionInfo":{"status":"ok","timestamp":1710586397568,"user_tz":-540,"elapsed":3,"user":{"displayName":"이진","userId":"07182137582732274273"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","memory_bank_path =  \"./memory_bank/resnet18\"  # memory bank를 저장할 경로\n","\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mn4hL4NY2xEy","executionInfo":{"status":"ok","timestamp":1710586397568,"user_tz":-540,"elapsed":3,"user":{"displayName":"이진","userId":"07182137582732274273"}},"outputId":"a3c54a64-a885-41ba-dc7e-17e51f6e1618"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["## PatchCore\n","- N: batch size\n","- N': memory bank size\n","- query: query size\n","- D: target_dim\n","- |A|: patch collection size\n","- |P|: patch_size"],"metadata":{"id":"Msx2Oo-q3BKS"}},{"cell_type":"code","source":["model = models.resnet18(weights = models.ResNet18_Weights.IMAGENET1K_V1)\n","\n","model.eval()\n","test = nn.Sequential(*list(model.children())[:-3])\n","model = model.to(device)\n","\n","# Patchcore Implementation - https://arxiv.org/abs/2106.08265\n","class PatchCore(nn.Module):\n","    def __init__(\n","        self,\n","        backbone,\n","        per_memory_bank_size=4,\n","        memory_bank_path=\"./memory_bank\",\n","        device=device,\n","        target_dim=384 * 3,  # |D|\n","        patch_size=3,  # |P|\n","        d=3,  # nearest\n","    ):\n","        \"\"\"\n","        Args:\n","            backbone: torch.nn.Module\n","            per_memory_bank_size: int\n","            memory_bank_path: str\n","            device: torch.device\n","            target_dim: int\n","            patch_size: int\n","            d: int\n","        \"\"\"\n","\n","        super().__init__()\n","        self.backbone = model\n","        self.backbone.eval()\n","\n","        self.layer2_output, self.layer3_output = None, None\n","        self.register_hook_for_layer2()  # register hook for layer2\n","        self.register_hook_for_layer3()  # register hook for layer3\n","\n","        self.memory_bank = None\n","        self.per_memory_bank_size = per_memory_bank_size\n","        self.memory_bank_path = memory_bank_path\n","        self.device = device\n","        self.target_dim = target_dim\n","        self.average_pool = nn.AdaptiveAvgPool1d(self.target_dim)\n","        self.patch_size = patch_size\n","        self.d = d\n","\n","    def register_hook_for_layer2(self):\n","        self.backbone.layer2.register_forward_hook(self._register_hook_for_layer2)\n","\n","    def _register_hook_for_layer2(self, module, input, output):  # (B, 128, 28, 28)\n","        layer2_output = output\n","        self.layer2_output = layer2_output  # (B, 128, 28, 28)\n","        # self.layer2_output = None\n","\n","    def register_hook_for_layer3(self):\n","        self.backbone.layer3.register_forward_hook(self._register_hook_for_layer3)\n","\n","    def _register_hook_for_layer3(self, module, input, output):  # (B, 256, 14, 14)\n","        layer3_output = output\n","        layer3_output = nn.functional.interpolate(\n","            layer3_output, scale_factor=2, mode=\"bilinear\"\n","        )\n","        self.layer3_output = layer3_output  # (B, 256, 28, 28)\n","\n","    def save_memory_bank(self, train_batch, file_name):\n","        \"\"\"\n","        training batch를 받아서 memory bank에 저장한다.\n","        \"\"\"\n","        self.backbone(train_batch)\n","        path = os.path.join(self.memory_bank_path, file_name)\n","        new_memory = self.patch_collection(self.patch_size)\n","        new_memory = new_memory.reshape(-1, new_memory.shape[-1])\n","        torch.save(new_memory, path)\n","\n","    def loader_memory_bank(self):\n","        \"\"\"\n","        memory bank를 이터레이터로 불러온다.\n","        \"\"\"\n","        file_list = os.listdir(self.memory_bank_path)\n","        file_list = [file for file in file_list if file.endswith(\".pth\")]\n","        file_list.sort()\n","        for file in file_list:\n","            path = os.path.join(self.memory_bank_path, file)\n","            new_memory = torch.load(path, map_location=self.device)\n","\n","            yield new_memory\n","\n","    def forward(self, x):\n","        self.backbone(x)\n","        query = self.patch_collection(self.patch_size)  # (N, |A|, D)\n","        l2 = torch.Tensor([]).to(device)\n","        for memory in self.loader_memory_bank():\n","            l2 = torch.cat((l2, self.cal_l2(query, memory)), dim=1)\n","            del memory\n","\n","        s_ = self.get_anomaly_score(l2)\n","        s = self.update_anomaly_score(s_, l2, self.d)\n","        return s\n","\n","    def get_anomaly_score(self, l2):\n","        \"\"\"\n","        Args:\n","            l2: (query, N', |A|)\n","        \"\"\"\n","        min_l2 = l2.min(dim=1).values  # (query, |A|)\n","        max_min_l2 = min_l2.max(dim=1).values  # (query, )\n","        return max_min_l2\n","\n","    def update_anomaly_score(self, s_, l2: torch.Tensor, d):\n","        \"\"\"\n","        Args:\n","            l2: (query, N', |A|)\n","            d = nearest\n","        \"\"\"\n","        m_train = l2.min(dim=1)  # (query, |A|)\n","        m_test = m_train.values.max(dim=1)  # (query)\n","        m_for_test = l2[:, :, m_test.indices]  # (query, N')\n","        m_train_nearest = m_for_test.topk(k=d, dim=1).values  # (query, d)\n","\n","        update_weight = 1 - (\n","            torch.exp(m_test.values) / torch.exp(m_train_nearest).sum(dim=1)\n","        )\n","        s = s_ * update_weight\n","        return s\n","\n","    def cal_l2(self, query, memory_bank):\n","        \"\"\"\n","        return: (query_size, memory_bank_size, |A|)\n","        \"\"\"\n","        memory_bank = memory_bank.unsqueeze(1)  # (N', 1, D)\n","        N, D = memory_bank.shape[0], memory_bank.shape[2]\n","        memory_bank = memory_bank.expand(N, query.shape[1], D)  # (N', |A|, D)\n","        l2 = []\n","        for q in query:\n","            q = q.unsqueeze(0)  # (1, |A|, D)\n","            diff = memory_bank - q  # (N', |A|, D)\n","            l2_ = diff.square().sum(dim=2)  # (N', |A|)\n","            l2_ = l2_.sqrt()\n","            l2.append(l2_)\n","            del q\n","        l2 = torch.stack(l2, dim=0)  # (query, N', |A|)\n","        return l2\n","\n","    def feature(self, h, w):\n","        \"\"\"\n","        return: (N, C)\n","        \"\"\"\n","        H, W = self.layer2_output.shape[2], self.layer2_output.shape[3]\n","        if not (0 <= h < H and 0 <= w < W):\n","            return torch.tensor([]).to(self.device)\n","        layer2 = self.layer2_output[:, :, h, w]  # (B, C)\n","\n","        # TODO: 아래 코드\n","        if self.layer3_output is not None:\n","            layer3 = self.layer3_output[:, :, h, w]  # (B, C')\n","        else:\n","            layer3 = torch.tensor([]).to(self.device)\n","\n","        feature = torch.cat((layer2, layer3), dim=1)\n","        return feature\n","\n","    def neighborhood_features(self, h, w, patch_size):\n","        \"\"\"\n","        return: (N, |P|, C) -> path_size x patch_size\n","        \"\"\"\n","        features = []\n","        for i in range(math.floor(-patch_size / 2), math.floor(patch_size / 2)):\n","            for j in range(math.floor(-patch_size / 2), math.floor(patch_size / 2)):\n","                feature = self.feature(h + i, w + j)\n","                if feature.shape[0] == 0:\n","                    continue\n","                features.append(feature)\n","        features = torch.stack(features, dim=1)\n","        return features\n","\n","    def patch(self, h, w, patch_size):\n","        \"\"\"\n","        return: (N, D)\n","        \"\"\"\n","        features = self.neighborhood_features(h, w, patch_size)\n","        features = features.permute(0, 2, 1)  # (N, C, |P|)\n","        features = features.reshape(features.shape[0], -1)  # (N, C X |P|)\n","        features = self.average_pool(features)  # (N, target_dim) = (N, D)\n","        return features\n","\n","    def patch_collection(self, patch_size):\n","        \"\"\"\n","        return: (N, |A|, D)\n","        \"\"\"\n","        H, W = self.layer2_output.shape[2], self.layer2_output.shape[3]\n","\n","        patch_collection = []\n","        # average pooling으로 나눌 수 있게 중심만 고려한다.\n","        for h in range(math.ceil(patch_size / 2), H - math.ceil(patch_size / 2)):\n","            for w in range(math.ceil(patch_size / 2), W - math.ceil(patch_size / 2)):\n","                patch = self.patch(h, w, patch_size)\n","                patch_collection.append(patch)\n","        patch_collection = torch.stack(patch_collection, dim=1)\n","        return patch_collection"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PDDZnI1U3AUS","executionInfo":{"status":"ok","timestamp":1710586398063,"user_tz":-540,"elapsed":498,"user":{"displayName":"이진","userId":"07182137582732274273"}},"outputId":"eebbd18d-08b8-4396-bf84-420975f0606e"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 169MB/s]\n"]}]},{"cell_type":"code","source":["params = {\n","    \"patch_size\": 5,\n","    \"per_memory_bank_size\": 1,\n","    \"d\": 2,\n","    \"target_dim\": 1920,\n","}"],"metadata":{"id":"1Ox_Zx0a4ybT","executionInfo":{"status":"ok","timestamp":1710586410178,"user_tz":-540,"elapsed":1,"user":{"displayName":"이진","userId":"07182137582732274273"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Finding Threshold of PatchCore\n","k = 13\n","\n","# Threshold\n","data = pd.read_csv(\"./train.csv\")\n","sum = 0.0\n","m = 213 // k\n","\n","\n","def save2(batch_size=1, config=dict()):  # Threshold 구하는 데 필요한 save 함수\n","    # memory_bank_path 폴더 초기화\n","    if os.path.exists(memory_bank_path):\n","        for file in os.listdir(memory_bank_path):\n","            os.remove(os.path.join(memory_bank_path, file))\n","    else:\n","        os.makedirs(memory_bank_path)\n","\n","    patch_core = PatchCore(model, memory_bank_path=memory_bank_path, **config)\n","    train_data = CustomDataset(csv_file=\"./train2.csv\", transform=transform)\n","    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n","\n","    for idx, x in enumerate(tqdm(train_loader)):\n","        x = x[0].to(device)\n","        patch_core.save_memory_bank(x, f\"memory_bank_{batch_size*idx}.pth\")\n","\n","\n","for i in range(m):\n","    j = i * k\n","    validation_indices = range(j, j + 13)  # 13개씩 슬라이싱\n","    validation_idx = data.loc[validation_indices]\n","    train_idx = data.drop(validation_indices)\n","\n","    train_idx.to_csv(\"./train2.csv\", index=False)\n","    validation_idx.to_csv(\"./validation.csv\", index=False)\n","\n","    save2(batch_size=1, config=params)\n","\n","    patch_core = PatchCore(model, memory_bank_path=memory_bank_path, **params)\n","    test_data = CustomDataset(csv_file=\"./validation.csv\", transform=transform)\n","    test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n","    anomaly_score = torch.tensor([], device=device)\n","    with torch.no_grad():\n","        for idx, x in tqdm(enumerate(test_loader), desc=\"query\"):\n","            x = x[0].to(device)\n","            l2 = patch_core.forward(x)\n","\n","            print(f\"l2: {l2}\")\n","\n","            anomaly_score = torch.cat([anomaly_score, l2], dim=0)\n","            anomaly_score_sorted_idx = anomaly_score.sort(descending=True).indices\n","\n","    anomaly_score = anomaly_score.cpu()\n","\n","    max_value, max_index = torch.max(anomaly_score, 0)\n","    print(f\"{i+1}th slice max value : {max_value.item():.4f} \\n\")\n","    sum += max_value.item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"gFzyvKhD5Tw9","executionInfo":{"status":"error","timestamp":1710587803040,"user_tz":-540,"elapsed":491,"user":{"displayName":"이진","userId":"07182137582732274273"}},"outputId":"9ee7ca59-9baa-493f-bcd0-e30216114132"},"execution_count":11,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: './train.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-128d265d1729>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m213\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './train.csv'"]}]},{"cell_type":"code","source":["Threshold = sum /m\n","Threshold"],"metadata":{"id":"pW3cMwmg-pdS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save(batch_size=1, config=dict()):  # Inference에 필요한 save 함수\n","    # memory_bank_path 폴더 초기화\n","    if os.path.exists(memory_bank_path):\n","        for file in os.listdir(memory_bank_path):\n","            os.remove(os.path.join(memory_bank_path, file))\n","    else:\n","        os.makedirs(memory_bank_path)\n","\n","    patch_core = PatchCore(model, memory_bank_path=memory_bank_path, **config)\n","    train_data = CustomDataset(csv_file=\"./train.csv\", transform=transform)\n","    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n","\n","    for idx, x in enumerate(tqdm(train_loader)):\n","        x = x[0].to(device)\n","        patch_core.save_memory_bank(x, f\"memory_bank_{batch_size*idx}.pth\")"],"metadata":{"id":"kA39XwVt_OTY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save(batch_size=1, config=params)\n","\n","patch_core = PatchCore(model, memory_bank_path=memory_bank_path, **params)\n","\n","test_data = CustomDataset(csv_file=\"./test.csv\", transform=transform)\n","test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n","anomaly_score = torch.tensor([], device=device)\n","with torch.no_grad():\n","    for idx, x in tqdm(enumerate(test_loader), desc=\"query\"):\n","        x = x[0].to(device)\n","        l2 = patch_core.forward(x)\n","\n","        # for i in range(l2.shape[0]):\n","        #     wandb.log({\"l2\": l2[i].item()})\n","        print(f\"l2: {l2}\")\n","\n","        anomaly_score = torch.cat([anomaly_score, l2], dim=0)\n","        anomaly_score_sorted_idx = anomaly_score.sort(descending=True).indices\n","\n","anomaly_score = anomaly_score.cpu()\n","plt.plot(anomaly_score)\n","plt.show()"],"metadata":{"id":"mty9USMD_Rr1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Threshold 보다 크면 이상치로 판별\n","data = anomaly_score\n","data = [idx for idx, d in enumerate(data) if d > Threshold]\n","outliers_idx1 = data\n","print(outliers_idx1)"],"metadata":{"id":"YkQ0woZ6_TxW"},"execution_count":null,"outputs":[]}]}