{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":850,"status":"ok","timestamp":1710586397568,"user":{"displayName":"이진","userId":"07182137582732274273"},"user_tz":-540},"id":"WTwapP_P1oBl"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from torchvision import models, transforms\n","from torch.utils.data import DataLoader, Dataset\n","from torchsummary import summary\n","from PIL import Image\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from tqdm import tqdm\n","import random\n","from tqdm import tqdm\n","import os\n","import matplotlib.pyplot as plt\n","import wandb\n","import logging\n","import math"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1710586397568,"user":{"displayName":"이진","userId":"07182137582732274273"},"user_tz":-540},"id":"IwnetAYi1v4n"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, csv_file, image_folder, transform=None):\n","        self.df = pd.read_csv(csv_file)\n","        self.image_folder = image_folder\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.df['img_path'].iloc[idx].split(\"/\")[-1]\n","        img_path = os.path.join(self.image_folder, img_name)\n","        image = Image.open(img_path)\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1710586397568,"user":{"displayName":"이진","userId":"07182137582732274273"},"user_tz":-540},"id":"SfD-IVRs2eFj"},"outputs":[],"source":["transform = transforms.Compose(\n","    [\n","        transforms.Resize((256, 256)),\n","        transforms.CenterCrop((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]),\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1710586397568,"user":{"displayName":"이진","userId":"07182137582732274273"},"user_tz":-540},"id":"mn4hL4NY2xEy","outputId":"a3c54a64-a885-41ba-dc7e-17e51f6e1618"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","memory_bank_path =  \"./memory_bank/resnet18\"  # memory bank를 저장할 경로\n","\n","device"]},{"cell_type":"markdown","metadata":{"id":"Msx2Oo-q3BKS"},"source":["## PatchCore\n","- N: batch size\n","- N': memory bank size\n","- query: query size\n","- D: target_dim\n","- |A|: patch collection size\n","- |P|: patch_size"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":498,"status":"ok","timestamp":1710586398063,"user":{"displayName":"이진","userId":"07182137582732274273"},"user_tz":-540},"id":"PDDZnI1U3AUS","outputId":"eebbd18d-08b8-4396-bf84-420975f0606e"},"outputs":[],"source":["model = models.resnet18(weights = models.ResNet18_Weights.IMAGENET1K_V1)\n","\n","model.eval()\n","test = nn.Sequential(*list(model.children())[:-3])\n","model = model.to(device)\n","\n","# Patchcore Implementation - https://arxiv.org/abs/2106.08265\n","class PatchCore(nn.Module):\n","    def __init__(\n","        self,\n","        backbone,\n","        per_memory_bank_size=4,\n","        memory_bank_path=\"./memory_bank\",\n","        device=device,\n","        target_dim=384 * 3,  # |D|\n","        patch_size=3,  # |P|\n","        d=3,  # nearest\n","    ):\n","        \"\"\"\n","        Args:\n","            backbone: torch.nn.Module\n","            per_memory_bank_size: int\n","            memory_bank_path: str\n","            device: torch.device\n","            target_dim: int\n","            patch_size: int\n","            d: int\n","        \"\"\"\n","\n","        super().__init__()\n","        self.backbone = model\n","        self.backbone.eval()\n","\n","        self.layer2_output, self.layer3_output = None, None\n","        self.register_hook_for_layer2()  # register hook for layer2\n","        self.register_hook_for_layer3()  # register hook for layer3\n","\n","        self.memory_bank = None\n","        self.per_memory_bank_size = per_memory_bank_size\n","        self.memory_bank_path = memory_bank_path\n","        self.device = device\n","        self.target_dim = target_dim\n","        self.average_pool = nn.AdaptiveAvgPool1d(self.target_dim)\n","        self.patch_size = patch_size\n","        self.d = d\n","\n","    def register_hook_for_layer2(self):\n","        self.backbone.layer2.register_forward_hook(self._register_hook_for_layer2)\n","\n","    def _register_hook_for_layer2(self, module, input, output):  # (B, 128, 28, 28)\n","        layer2_output = output\n","        self.layer2_output = layer2_output  # (B, 128, 28, 28)\n","        # self.layer2_output = None\n","\n","    def register_hook_for_layer3(self):\n","        self.backbone.layer3.register_forward_hook(self._register_hook_for_layer3)\n","\n","    def _register_hook_for_layer3(self, module, input, output):  # (B, 256, 14, 14)\n","        layer3_output = output\n","        layer3_output = nn.functional.interpolate(\n","            layer3_output, scale_factor=2, mode=\"bilinear\"\n","        )\n","        self.layer3_output = layer3_output  # (B, 256, 28, 28)\n","\n","    def save_memory_bank(self, train_batch, file_name):\n","        \"\"\"\n","        training batch를 받아서 memory bank에 저장한다.\n","        \"\"\"\n","        self.backbone(train_batch)\n","        path = os.path.join(self.memory_bank_path, file_name)\n","        new_memory = self.patch_collection(self.patch_size)\n","        new_memory = new_memory.reshape(-1, new_memory.shape[-1])\n","        torch.save(new_memory, path)\n","\n","    def loader_memory_bank(self):\n","        \"\"\"\n","        memory bank를 이터레이터로 불러온다.\n","        \"\"\"\n","        file_list = os.listdir(self.memory_bank_path)\n","        file_list = [file for file in file_list if file.endswith(\".pth\")]\n","        file_list.sort()\n","        for file in file_list:\n","            path = os.path.join(self.memory_bank_path, file)\n","            new_memory = torch.load(path, map_location=self.device)\n","\n","            yield new_memory\n","\n","    def forward(self, x):\n","        self.backbone(x)\n","        query = self.patch_collection(self.patch_size)  # (N, |A|, D)\n","        l2 = torch.Tensor([]).to(device)\n","        for memory in self.loader_memory_bank():\n","            l2 = torch.cat((l2, self.cal_l2(query, memory)), dim=1)\n","            del memory\n","\n","        s_ = self.get_anomaly_score(l2)\n","        s = self.update_anomaly_score(s_, l2, self.d)\n","        return s\n","\n","    def get_anomaly_score(self, l2):\n","        \"\"\"\n","        Args:\n","            l2: (query, N', |A|)\n","        \"\"\"\n","        min_l2 = l2.min(dim=1).values  # (query, |A|)\n","        max_min_l2 = min_l2.max(dim=1).values  # (query, )\n","        return max_min_l2\n","\n","    def update_anomaly_score(self, s_, l2: torch.Tensor, d):\n","        \"\"\"\n","        Args:\n","            l2: (query, N', |A|)\n","            d = nearest\n","        \"\"\"\n","        m_train = l2.min(dim=1)  # (query, |A|)\n","        m_test = m_train.values.max(dim=1)  # (query)\n","        m_for_test = l2[:, :, m_test.indices]  # (query, N')\n","        m_train_nearest = m_for_test.topk(k=d, dim=1).values  # (query, d)\n","\n","        update_weight = 1 - (\n","            torch.exp(m_test.values) / torch.exp(m_train_nearest).sum(dim=1)\n","        )\n","        s = s_ * update_weight\n","        return s\n","\n","    def cal_l2(self, query, memory_bank):\n","        \"\"\"\n","        return: (query_size, memory_bank_size, |A|)\n","        \"\"\"\n","        memory_bank = memory_bank.unsqueeze(1)  # (N', 1, D)\n","        N, D = memory_bank.shape[0], memory_bank.shape[2]\n","        memory_bank = memory_bank.expand(N, query.shape[1], D)  # (N', |A|, D)\n","        l2 = []\n","        for q in query:\n","            q = q.unsqueeze(0)  # (1, |A|, D)\n","            diff = memory_bank - q  # (N', |A|, D)\n","            l2_ = diff.square().sum(dim=2)  # (N', |A|)\n","            l2_ = l2_.sqrt()\n","            l2.append(l2_)\n","            del q\n","        l2 = torch.stack(l2, dim=0)  # (query, N', |A|)\n","        return l2\n","\n","    def feature(self, h, w):\n","        \"\"\"\n","        return: (N, C)\n","        \"\"\"\n","        H, W = self.layer2_output.shape[2], self.layer2_output.shape[3]\n","        if not (0 <= h < H and 0 <= w < W):\n","            return torch.tensor([]).to(self.device)\n","        layer2 = self.layer2_output[:, :, h, w]  # (B, C)\n","\n","        # TODO: 아래 코드\n","        if self.layer3_output is not None:\n","            layer3 = self.layer3_output[:, :, h, w]  # (B, C')\n","        else:\n","            layer3 = torch.tensor([]).to(self.device)\n","\n","        feature = torch.cat((layer2, layer3), dim=1)\n","        return feature\n","\n","    def neighborhood_features(self, h, w, patch_size):\n","        \"\"\"\n","        return: (N, |P|, C) -> path_size x patch_size\n","        \"\"\"\n","        features = []\n","        for i in range(math.floor(-patch_size / 2), math.floor(patch_size / 2)):\n","            for j in range(math.floor(-patch_size / 2), math.floor(patch_size / 2)):\n","                feature = self.feature(h + i, w + j)\n","                if feature.shape[0] == 0:\n","                    continue\n","                features.append(feature)\n","        features = torch.stack(features, dim=1)\n","        return features\n","\n","    def patch(self, h, w, patch_size):\n","        \"\"\"\n","        return: (N, D)\n","        \"\"\"\n","        features = self.neighborhood_features(h, w, patch_size)\n","        features = features.permute(0, 2, 1)  # (N, C, |P|)\n","        features = features.reshape(features.shape[0], -1)  # (N, C X |P|)\n","        features = self.average_pool(features)  # (N, target_dim) = (N, D)\n","        return features\n","\n","    def patch_collection(self, patch_size):\n","        \"\"\"\n","        return: (N, |A|, D)\n","        \"\"\"\n","        H, W = self.layer2_output.shape[2], self.layer2_output.shape[3]\n","\n","        patch_collection = []\n","        # average pooling으로 나눌 수 있게 중심만 고려한다.\n","        for h in range(math.ceil(patch_size / 2), H - math.ceil(patch_size / 2)):\n","            for w in range(math.ceil(patch_size / 2), W - math.ceil(patch_size / 2)):\n","                patch = self.patch(h, w, patch_size)\n","                patch_collection.append(patch)\n","        patch_collection = torch.stack(patch_collection, dim=1)\n","        return patch_collection"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1710586410178,"user":{"displayName":"이진","userId":"07182137582732274273"},"user_tz":-540},"id":"1Ox_Zx0a4ybT"},"outputs":[],"source":["params = {\n","    \"patch_size\": 5,\n","    \"per_memory_bank_size\": 1,\n","    \"d\": 2,\n","    \"target_dim\": 1920,\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"elapsed":491,"status":"error","timestamp":1710587803040,"user":{"displayName":"이진","userId":"07182137582732274273"},"user_tz":-540},"id":"gFzyvKhD5Tw9","outputId":"9ee7ca59-9baa-493f-bcd0-e30216114132"},"outputs":[],"source":["# Finding Threshold of PatchCore\n","k = 13\n","\n","# Threshold\n","data = pd.read_csv(\"./train.csv\")\n","sum = 0.0\n","m = 213 // k\n","\n","\n","def save2(batch_size=1, config=dict()):  # Threshold 구하는 데 필요한 save 함수\n","    # memory_bank_path 폴더 초기화\n","    if os.path.exists(memory_bank_path):\n","        for file in os.listdir(memory_bank_path):\n","            os.remove(os.path.join(memory_bank_path, file))\n","    else:\n","        os.makedirs(memory_bank_path)\n","\n","    patch_core = PatchCore(model, memory_bank_path=memory_bank_path, **config)\n","    train_data = CustomDataset(csv_file=\"./train2.csv\", transform=transform)\n","    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n","\n","    for idx, x in enumerate(tqdm(train_loader)):\n","        x = x[0].to(device)\n","        patch_core.save_memory_bank(x, f\"memory_bank_{batch_size*idx}.pth\")\n","\n","\n","for i in range(m):\n","    j = i * k\n","    validation_indices = range(j, j + 13)  # 13개씩 슬라이싱\n","    validation_idx = data.loc[validation_indices]\n","    train_idx = data.drop(validation_indices)\n","\n","    train_idx.to_csv(\"./train2.csv\", index=False)\n","    validation_idx.to_csv(\"./validation.csv\", index=False)\n","\n","    save2(batch_size=1, config=params)\n","\n","    patch_core = PatchCore(model, memory_bank_path=memory_bank_path, **params)\n","    test_data = CustomDataset(csv_file=\"./validation.csv\", transform=transform)\n","    test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n","    anomaly_score = torch.tensor([], device=device)\n","    with torch.no_grad():\n","        for idx, x in tqdm(enumerate(test_loader), desc=\"query\"):\n","            x = x[0].to(device)\n","            l2 = patch_core.forward(x)\n","\n","            print(f\"l2: {l2}\")\n","\n","            anomaly_score = torch.cat([anomaly_score, l2], dim=0)\n","            anomaly_score_sorted_idx = anomaly_score.sort(descending=True).indices\n","\n","    anomaly_score = anomaly_score.cpu()\n","\n","    max_value, max_index = torch.max(anomaly_score, 0)\n","    print(f\"{i+1}th slice max value : {max_value.item():.4f} \\n\")\n","    sum += max_value.item()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pW3cMwmg-pdS"},"outputs":[],"source":["Threshold = sum /m\n","Threshold"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kA39XwVt_OTY"},"outputs":[],"source":["def save(batch_size=1, config=dict()):  # Inference에 필요한 save 함수\n","    # memory_bank_path 폴더 초기화\n","    if os.path.exists(memory_bank_path):\n","        for file in os.listdir(memory_bank_path):\n","            os.remove(os.path.join(memory_bank_path, file))\n","    else:\n","        os.makedirs(memory_bank_path)\n","\n","    patch_core = PatchCore(model, memory_bank_path=memory_bank_path, **config)\n","    train_data = CustomDataset(csv_file=\"./train.csv\", transform=transform)\n","    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n","\n","    for idx, x in enumerate(tqdm(train_loader)):\n","        x = x[0].to(device)\n","        patch_core.save_memory_bank(x, f\"memory_bank_{batch_size*idx}.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mty9USMD_Rr1"},"outputs":[],"source":["save(batch_size=1, config=params)\n","\n","patch_core = PatchCore(model, memory_bank_path=memory_bank_path, **params)\n","\n","test_data = CustomDataset(csv_file=\"./test.csv\", transform=transform)\n","test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n","anomaly_score = torch.tensor([], device=device)\n","with torch.no_grad():\n","    for idx, x in tqdm(enumerate(test_loader), desc=\"query\"):\n","        x = x[0].to(device)\n","        l2 = patch_core.forward(x)\n","\n","        # for i in range(l2.shape[0]):\n","        #     wandb.log({\"l2\": l2[i].item()})\n","        print(f\"l2: {l2}\")\n","\n","        anomaly_score = torch.cat([anomaly_score, l2], dim=0)\n","        anomaly_score_sorted_idx = anomaly_score.sort(descending=True).indices\n","\n","anomaly_score = anomaly_score.cpu()\n","plt.plot(anomaly_score)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YkQ0woZ6_TxW"},"outputs":[],"source":["# Threshold 보다 크면 이상치로 판별\n","data = anomaly_score\n","data = [idx for idx, d in enumerate(data) if d > Threshold]\n","outliers_idx1 = data\n","print(outliers_idx1)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN9kysBqEC5TGE3+2uPph0K","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.2"}},"nbformat":4,"nbformat_minor":0}
